{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66bf7e4",
   "metadata": {},
   "source": [
    "### ë¬¸ì œ 6-1 : ì¡°ê±´ë¶€ ë¶„ê¸°ê°€ ìˆëŠ” ë©”ë‰´ ì¶”ì²œ ì‹œìŠ¤í…œ ( LangGraph ì‚¬ìš©í•˜ê¸°)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36be364b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë²¡í„° DBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
      "Ollama Embeddings ëª¨ë¸ (bge-m3)ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DBë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\AppData\\Local\\Temp\\ipykernel_16272\\2940869297.py:78: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"bge-m3\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMìœ¼ë¡œ Groq ëª¨ë¸ (llama3-8b-8192)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "--- â˜•ï¸ ì¹´í˜ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ---\n",
      "ë©”ë‰´, ê°€ê²©, ì¶”ì²œ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ê°€ ëë‚©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, TypedDict, Annotated\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# LangChain ê´€ë ¨\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# LangGraph ê´€ë ¨\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# LangChain Community ë° ê¸°íƒ€\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---\n",
    "load_dotenv()\n",
    "\n",
    "# --- ì‚¬ìš©ì ìƒíƒœ ì •ì˜ ---\n",
    "class CafeState(TypedDict):\n",
    "    \"\"\"\n",
    "    ì¹´í˜ ì±—ë´‡ì˜ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    'messages' í•„ë“œëŠ” add_messagesë¥¼ í†µí•´ LangGraphê°€ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    question_type: str | None = None  # ë¶„ë¥˜ëœ ì§ˆë¬¸ ìœ í˜•\n",
    "\n",
    "\n",
    "# --- ë²¡í„° DB ìƒì„± ë° ë¡œë“œ ---\n",
    "DATA_DIR = \"./data\"\n",
    "DB_PATH = \"./db/cafe_db\"\n",
    "MENU_DATA_FILE = os.path.join(DATA_DIR, \"cafe_menu_data.txt\")\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "if not os.path.exists(MENU_DATA_FILE):\n",
    "    print(f\"âš ï¸ '{MENU_DATA_FILE}' íŒŒì¼ì´ ì—†ì–´ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\")\n",
    "    with open(MENU_DATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"ì•„ë©”ë¦¬ì¹´ë…¸: ì‹ ì„ í•œ ì—ìŠ¤í”„ë ˆì†Œì— ë¬¼ì„ ë”í•œ í´ë˜ì‹ ì»¤í”¼. â‚©4,000\\n\")\n",
    "        f.write(\"ì¹´í˜ ë¼ë–¼: ë¶€ë“œëŸ¬ìš´ ìš°ìœ ì™€ ì—ìŠ¤í”„ë ˆì†Œì˜ ì¡°í™”. â‚©4,500\\n\")\n",
    "        f.write(\"ë°”ë‹ë¼ ë¼ë–¼: ë‹¬ì½¤í•œ ë°”ë‹ë¼ ì‹œëŸ½ì´ ë“¤ì–´ê°„ ë¼ë–¼. â‚©5,000\\n\")\n",
    "        f.write(\"ì¹´í‘¸ì¹˜ë…¸: í’ì„±í•œ ìš°ìœ  ê±°í’ˆì´ ì˜¬ë¼ê°„ ì»¤í”¼. â‚©4,500\\n\")\n",
    "        f.write(\"ì´ˆì½œë¦¿ ë¼ë–¼: ì§„í•œ ì´ˆì½œë¦¿ê³¼ ìš°ìœ ê°€ ë§Œë‚œ ìŒë£Œ. â‚©5,500\\n\")\n",
    "        f.write(\"ë”¸ê¸° ìŠ¤ë¬´ë””: ì‹ ì„ í•œ ë”¸ê¸°ë¡œ ë§Œë“  ìƒí¼í•œ ìŠ¤ë¬´ë””. â‚©6,000\\n\")\n",
    "        f.write(\"ë¸”ë£¨ë² ë¦¬ ë² ì´ê¸€: í¬ë¦¼ì¹˜ì¦ˆì™€ ì˜ ì–´ìš¸ë¦¬ëŠ” ë¸”ë£¨ë² ë¦¬ ë² ì´ê¸€. â‚©3,500\\n\")\n",
    "        f.write(\"ì¹˜ì¦ˆ ì¼€ì´í¬: ë¶€ë“œëŸ½ê³  ì§„í•œ ì¹˜ì¦ˆì˜ ë§›. â‚©6,500\\n\")\n",
    "        f.write(\"ì—ê·¸ ìƒŒë“œìœ„ì¹˜: ì‹ ì„ í•œ ì¬ë£Œë¡œ ë§Œë“  ë“ ë“ í•œ ìƒŒë“œìœ„ì¹˜. â‚©5,800\\n\")\n",
    "    \n",
    "    if os.path.exists(DB_PATH):\n",
    "        shutil.rmtree(DB_PATH)\n",
    "    print(\"âœ… ì˜ˆì‹œ ë°ì´í„° íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ë²¡í„° DBë¥¼ ìƒˆë¡œ ìƒì„±í•˜ë ¤ë©´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "# ë²¡í„° DBê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³ , ìˆìœ¼ë©´ ë¡œë“œ\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(\"âœ… ë²¡í„° DBê°€ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "    os.makedirs(DB_PATH, exist_ok=True)\n",
    "    loader = TextLoader(MENU_DATA_FILE, encoding=\"utf-8\")\n",
    "    docs = loader.load()\n",
    "    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "    db = FAISS.from_documents(chunks, embeddings)\n",
    "    db.save_local(DB_PATH)\n",
    "    print(\"âœ… ë²¡í„° DB ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(\"âœ… ë²¡í„° DBê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"bge-m3\")\n",
    "menu_db = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "print(f\"Ollama Embeddings ëª¨ë¸ (bge-m3)ì„ ì‚¬ìš©í•˜ì—¬ ë²¡í„° DBë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# --- ì •ë³´ ì¶”ì¶œ í•¨ìˆ˜ ---\n",
    "def extract_menu_info(doc: Document) -> dict:\n",
    "    \"\"\"Vector DB ë¬¸ì„œì—ì„œ êµ¬ì¡°í™”ëœ ë©”ë‰´ ì •ë³´ (ì´ë¦„, ê°€ê²©, ì„¤ëª…)ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    content = doc.page_content\n",
    "    name_match = re.match(r'([^:]+):', content)\n",
    "    menu_name = name_match.group(1).strip() if name_match else 'Unknown'\n",
    "    price_match = re.search(r'â‚©([\\d,]+)', content)\n",
    "    price = price_match.group(0) if price_match else \"ê°€ê²© ì •ë³´ ì—†ìŒ\"\n",
    "    description = content\n",
    "    if name_match:\n",
    "        description = description.replace(name_match.group(0), \"\", 1).strip()\n",
    "    if price_match:\n",
    "        description = description.replace(price, \"\").strip()\n",
    "    if description.startswith(\":\"):\n",
    "        description = description[1:].strip()\n",
    "    return {\"name\": menu_name, \"price\": price, \"description\": description or \"ì„¤ëª… ì—†ìŒ\"}\n",
    "\n",
    "# --- LLM ì´ˆê¸°í™” (Groq ì‚¬ìš©) ---\n",
    "llm = ChatGroq(model_name=\"llama3-8b-8192\", temperature=0.7)\n",
    "print(f\"LLMìœ¼ë¡œ Groq ëª¨ë¸ ({llm.model_name})ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# --- ì‘ë‹µ ìƒì„± ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---\n",
    "def generate_llm_response(current_messages: List[BaseMessage], context: str, user_query: str) -> str:\n",
    "    \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì¿¼ë¦¬, ëŒ€í™” ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ LLM ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    recent_history = current_messages[-5:]\n",
    "    messages_for_llm = recent_history + [\n",
    "        HumanMessage(content=f\"ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³ ê°ì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì •ë³´:\\n{context}\\n\\nê³ ê° ì§ˆë¬¸: {user_query}\")\n",
    "    ]\n",
    "    response = llm.invoke(messages_for_llm)\n",
    "    return response.content\n",
    "\n",
    "# --- ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---\n",
    "\n",
    "def classify_question_node(state: CafeState) -> dict:\n",
    "    \"\"\"ì‚¬ìš©ì ë¬¸ì˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì—¬ stateì˜ question_typeì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\"\"\"\n",
    "    last_message_content = state['messages'][-1].content.lower()\n",
    "    \n",
    "    if \"ê°€ê²©\" in last_message_content or \"ì–¼ë§ˆ\" in last_message_content:\n",
    "        q_type = \"price\"\n",
    "    elif \"ì¶”ì²œ\" in last_message_content or \"ì¢‹ì€\" in last_message_content or \"ì¸ê¸°\" in last_message_content:\n",
    "        q_type = \"recommend\"\n",
    "    elif any(keyword in last_message_content for keyword in [\"ë©”ë‰´\", \"ë””ì €íŠ¸\", \"ë¼ë–¼\", \"ë­\", \"ì–´ë–¤\", \"ì¢…ë¥˜\", \"ìˆë‚˜ìš”\"]):\n",
    "        q_type = \"menu\"\n",
    "    else:\n",
    "        q_type = \"general\"\n",
    "    # ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "    return {\"question_type\": q_type}\n",
    "\n",
    "def route_question(state: CafeState) -> str:\n",
    "    \"\"\"stateì— ì €ì¥ëœ question_typeì— ë”°ë¼ ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œ ì´ë¦„ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return state[\"question_type\"]\n",
    "\n",
    "def answer_price(state: CafeState) -> dict:\n",
    "    query = state['messages'][-1].content\n",
    "    docs = menu_db.similarity_search(query + \" ê°€ê²©\", k=5)\n",
    "    items = [extract_menu_info(doc) for doc in docs]\n",
    "    if items:\n",
    "        context_str = \"\\n\".join([f\"- {i['name']}: {i['price']} ({i['description']})\" for i in items])\n",
    "        response_content = generate_llm_response(state['messages'], context_str, query)\n",
    "    else:\n",
    "        response_content = \"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ë©”ë‰´ì˜ ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ë©”ë‰´ì˜ ê°€ê²©ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?\"\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "def answer_recommend(state: CafeState) -> dict:\n",
    "    query = state['messages'][-1].content\n",
    "    docs = menu_db.similarity_search(query, k=3)\n",
    "    if not docs:\n",
    "        docs = menu_db.similarity_search(\"ì¸ê¸° ë©”ë‰´\", k=3)\n",
    "    items = [extract_menu_info(doc) for doc in docs]\n",
    "    if items:\n",
    "        context_str = \"\\n\".join([f\"ì¶”ì²œ ë©”ë‰´: {i['name']} - {i['description']} ({i['price']})\" for i in items])\n",
    "        response_content = generate_llm_response(state['messages'], context_str, query)\n",
    "    else:\n",
    "        response_content = \"ì§€ê¸ˆ ì¶”ì²œí•´ ë“œë¦´ ë§Œí•œ íŠ¹ë³„í•œ ë©”ë‰´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ì¢…ë¥˜ì˜ ë©”ë‰´ë¥¼ ì„ í˜¸í•˜ì‹œë‚˜ìš”?\"\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "def answer_menu(state: CafeState) -> dict:\n",
    "    query = state['messages'][-1].content\n",
    "    docs = menu_db.similarity_search(query, k=4)\n",
    "    if not docs:\n",
    "        response_content = \"ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ë©”ë‰´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë©”ë‰´ ì´ë¦„ì„ ì •í™•íˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?\"\n",
    "    else:\n",
    "        items = [extract_menu_info(doc) for doc in docs]\n",
    "        context_str = \"\\n\".join([f\"{i['name']}: {i['description']} ({i['price']})\" for i in items])\n",
    "        response_content = generate_llm_response(state['messages'], context_str, query)\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "def fallback_answer(state: CafeState) -> dict:\n",
    "    query = state['messages'][-1].content\n",
    "    response_content = generate_llm_response(state['messages'], \"íŠ¹ë³„í•œ ë©”ë‰´ ì •ë³´ ì—†ìŒ\", query)\n",
    "    return {\"messages\": [AIMessage(content=response_content)]}\n",
    "\n",
    "\n",
    "# --- LangGraph ê·¸ë˜í”„ êµ¬ì„± (ìˆ˜ì •ëœ ë¶€ë¶„) ---\n",
    "graph = StateGraph(CafeState)\n",
    "\n",
    "# ê° ë…¸ë“œë¥¼ ë¬¸ìì—´ ì´ë¦„ê³¼ í•¨ê»˜ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "graph.add_node(\"classify\", classify_question_node)\n",
    "graph.add_node(\"price\", answer_price)\n",
    "graph.add_node(\"recommend\", answer_recommend)\n",
    "graph.add_node(\"menu\", answer_menu)\n",
    "graph.add_node(\"general\", fallback_answer)\n",
    "\n",
    "# ì§„ì…ì ì„ 'classify' ë…¸ë“œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "graph.set_entry_point(\"classify\")\n",
    "\n",
    "# 'classify' ë…¸ë“œì—ì„œ ì‹œì‘í•˜ëŠ” ì¡°ê±´ë¶€ ì—£ì§€ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "graph.add_conditional_edges(\n",
    "    source=\"classify\",\n",
    "    path=route_question,\n",
    "    path_map={\n",
    "        \"price\": \"price\",\n",
    "        \"recommend\": \"recommend\",\n",
    "        \"menu\": \"menu\",\n",
    "        \"general\": \"general\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# ê° ì‘ë‹µ ë…¸ë“œëŠ” ëŒ€í™”ë¥¼ ì¢…ë£Œ(END)í•©ë‹ˆë‹¤.\n",
    "graph.add_edge(\"price\", END)\n",
    "graph.add_edge(\"recommend\", END)\n",
    "graph.add_edge(\"menu\", END)\n",
    "graph.add_edge(\"general\", END)\n",
    "\n",
    "\n",
    "# --- ê·¸ë˜í”„ ì»´íŒŒì¼ ---\n",
    "app = graph.compile()\n",
    "\n",
    "# --- ì±—ë´‡ ì‹¤í–‰ ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- â˜•ï¸ ì¹´í˜ ì±—ë´‡ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ---\")\n",
    "    print(\"ë©”ë‰´, ê°€ê²©, ì¶”ì²œ ë“±ì— ëŒ€í•´ ë¬¼ì–´ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ì¢…ë£Œ'ë¼ê³  ì…ë ¥í•˜ë©´ ëŒ€í™”ê°€ ëë‚©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    messages = []\n",
    "    while True:\n",
    "        user_input = input(\"\\nğŸ’â€â™‚ï¸ ê³ ê°ë‹˜: \")\n",
    "        if user_input.lower() == \"ì¢…ë£Œ\":\n",
    "            print(\"ğŸ‘‹ ì±—ë´‡: ë‹¤ìŒì— ë˜ ë§Œë‚˜ìš”!\")\n",
    "            break\n",
    "\n",
    "        input_message = HumanMessage(content=user_input)\n",
    "        \n",
    "        result = app.invoke({\"messages\": messages + [input_message]})\n",
    "\n",
    "        ai_response_message = result[\"messages\"][-1]\n",
    "        print(f\"ğŸ¤– ì±—ë´‡: {ai_response_message.content}\")\n",
    "        \n",
    "        # ë‹¤ìŒ ëŒ€í™”ë¥¼ ìœ„í•´ ì „ì²´ ë©”ì‹œì§€ ì´ë ¥ ì—…ë°ì´íŠ¸\n",
    "        messages = result[\"messages\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
